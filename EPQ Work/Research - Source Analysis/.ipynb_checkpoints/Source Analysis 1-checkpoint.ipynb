{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bd5bc0-bd07-4ae3-8e35-6a04e55969d7",
   "metadata": {},
   "source": [
    "<div align=\"left\"><table border=\"19\" align=left>\n",
    " <tr>\n",
    "    <td>Type of source</td>\n",
    "    <td>Video Essay series</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Publication Date</td>\n",
    "    <td>October 2017</td>\n",
    " </tr>\n",
    "  <tr>\n",
    "    <td>Author</td>\n",
    "    <td>\"3Blue1Brown\", aka. Grant Sanderson</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Key Quotes</td>\n",
    "    <td></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Purpose and significance</td>\n",
    "    <td>This video series functions as a basic introduction to the concept of artifical intelligence. It takes the viewer from a position of basic computational knowledge to understanding a basic neural network very well. The purpose of this was to introduce me to the concept of a neural network and to set the foundations for my reasearch into the topic. It also goes into how you would construct a basic artificial intelligence. This is significantt as this process will likely make up a significant portion of my research for my epq. Also, having foundation knowledge of artifical intelligence is integral to my whole project.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Main Ideas</td>\n",
    "    <td>\n",
    "        <ul><li>The main focus of episode one is understanding why the networks are structured in layers, and begins to talk about construction of the input layer. It also talks about finalising the process using sigmoid functions (noting the discussion of ReLU as an alternative), as well as discussing notation which will be extremely helpful in undertstanding other sources.\n",
    "            </li><li>The main focus of episode two is how a network actually begins to train and learn. It discusses the cost function, and finding the minimum of the cost function by tweaking nodes (the process we often refer to as training or learning). It covers the calculus behind minimising $C(\\omega)$ by using the gradient method to find local minima. It breifly covers Î·, the learning rate aswell. \n",
    "     Episode three focuses on analysing and understanding a trained neural network. This lesson mainly cements topics in the first video, specifically the concept of assigning a weight to each pixel in each node, making a small picture that recognises patterns.\n",
    "      </li><li>Episode four focuses on a concept called \"backpropogation\". This video was very significant as the concept is a very difficult to understand one, and viewing it in this format really helped my understanding of backpropogation.\n",
    "      </li><li>Episode five is an extension of episode four, and looks more closely at backpropogation from a calculus standpoint. This lower-level understanding will likely be useful later on. </li></ul></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>What research questions does it answer?</td>\n",
    "    <td>It answers many questions about construction, training and analysis of a multi-layer perceptron network, giving a good foundation in arrtifical intelligence as a general concept. This means I can use that knowledge gained from this series and apply it to further research, in order to get the best understanding of a very wide and varied topic.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Do you agree?</td>\n",
    "    <td>n/a, source does not provide an opinion.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>What needs further research</td>\n",
    "   <td>I need to research other approaches to deep learning that are more suited to my particular linguistic application of deep learning, so that i have a fuller picture of other concepts such as RNNs, CNN, and other methods of training an ai that may be more suited to the project.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>References (hyperlink this)</td>\n",
    "    <td>The Series <a href=\"https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\">[0]</a>\n",
    "        \n",
    "        This github repository contained the code examples <a href=\"https://github.com/mnielsen/neural-networks-and-deep-learning\">[1]</a>\n",
    "        https://arxiv.org/abs/1412.0233\n",
    "        These Articles <a href=\"https://arxiv.org/abs/1611.03530\">[2]</a><a href=\"https://arxiv.org/abs/1706.05394\">[3]</a><a href=\"https://arxiv.org/abs/1412.0233\">[4]</a></td>\n",
    " </tr>\n",
    "  <tr>\n",
    "    <td>Is there anything missing or wrong with the source?</td>\n",
    "   <td>The source only covers a form of deep learning called a multi layer perceptron, meaning there is much research to be done on other approaches to machine learning.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Evaluation</td>\n",
    "    <td>Overall, I belive this source is crucial to my project as it lays a foundation of understanding incredibly well, allowing me to research more complex methods of machine learning to improve my project</td>\n",
    " </tr>\n",
    "  <tr>    \n",
    "    <td>Conclusion</td>\n",
    "    <td>In conclusion, I belive this source is integral to the success of my project as without it i would lack the foundation knowledge for further, and more cutting-edge research into the feild of deep learing.</td>\n",
    " </tr>\n",
    "</table></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c7fb7-5c0d-49f5-b3e4-7bb744b597e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edee3a-86dc-4f44-aea7-08b0416099c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
